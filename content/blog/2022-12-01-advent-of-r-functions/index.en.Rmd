---
title: Advent of R Functions
author: Dr. Mowinckel
date: '2022-12-01'
slug: 2022-12-01-advent-of-r-functions
categories: []
tags:
  - Advent calendar
  - R
  - R functions 2022
image: 'featured.jpg'
---

```{r, include=FALSE}
rows <- 30
options(max.print = rows)
knitr::opts_chunk$set(echo = TRUE, rows.print=rows)
```

It is advent!
And we all know by now how much I LOVE advent and Christmas. 
Keeping true to who I am and that I finally have some extra energy for things like this, this advent brings you a series of 24 pieces of code I often use in my work, and that I hope can be of interest and help to you.

## 1<sup>st</sup> of December - Creating a directory
I often find myself doing quite some file handling in my work, often leading to many of the same things happening over and over in slightly different contexts. 
If you try to create files in a directory that does not exists, R will throw an error

```{r error = TRUE}
penguins <- palmerpenguins::penguins
write.table(penguins, "new_folder/penguins.csv")
```

So, I very often have this piece of code in any file-writing function I make to create the folder. 
But R will also throw annoying warnings if the folder already exists, which I also don't like.

```{r warning = TRUE}
dir.create("new_folder") # no warning
dir.create("new_folder") # produces warning
```

```{r, include=FALSE}
unlink("new_folder", force = TRUE, recursive = TRUE)
```

The solution, is to check if the directory already exists, and make it if it does not.


```{r}
if(!dir.exists("new_folder")) dir.create("new_folder")
```

```{r, include=FALSE}
unlink("new_folder", force = TRUE, recursive = TRUE)
```
Actually, I often end up using a little convenience function for this, since I do it quite often.

```{r}
dir_create <- function(x, ...){
  if(!dir.exists(x)) 
    dir.create(x, recursive = TRUE, ...)
}
dir_create("new_folder")
dir_create("new_folder")
```

```{r, include=FALSE}
unlink("new_folder", force = TRUE, recursive = TRUE)
```
Here, I can have a function to easily create new folders, with any extra arguments to `dir.create` passed along using the `...` (ellipsis), and only make the directory if it does not already exist.
This is a staple bit of code for me.
Hope it will help you get your scripts tidier!


## 2<sup>nd</sup> of December - Writing subsetted data to files

I'll continue in the same line as the first day, with working with the file system.
I've shown how I create a utility function to create new directories if they don't exist, and now we want to write files to them!

I'll continue using base-R, as for this first part of the calendar, I am emulating work I do on our offline server where I often struggle with getting dependencies installed in stable ways. 

We have our lovely penguins data set, and I want to save one file per penguin species in the data.table.
That is, I want to split the data.frame into three data.frames each containing only the data from a single penguin species.
Then I want to save each of those to file. 

First we need to split the data set. 
Usually, when not on the server, I'd do some {dplyr} `nest_by` magic, but I cannot in this case.
So we need to deal with what we have.
Neatly, base-R has the `split` function, which does exactly what I want.

```{r}
penguins |> 
  split(~species)
```
Out comes three data.frames with each data set in them, preserved in a list. 
Awesome!

Then, I need to save them to files. 
I will use `lapply` (list apply) to loop through the list, and save each file.
I send each data set into the lapply function, giving them the placeholder name `x`.
So, `x` will be one data.frame. 
Then I write the csv to file, using the species name.

```{r}
penguins |> 
  split(~species) |> 
  lapply(function(x){
    write.csv(x, paste0(unique(x$species), ".csv"), row.names = FALSE)
  })

list.files(".", "csv")
```
```{r, include=FALSE}
file.remove(list.files(".", "csv"))
```
Ok. so the files are there, but I am not super happy with this. 
I don't like capitalisation in my file names, and they are not in a folder. 
I also cannot easily change the grouping factor, if I for instance wanted to save by island or sex in stead.
To do that, I'll construct a function that will do my work for me in a standardised way.
It's going to be quite a doozy, but its such a convenient thing for me!

```{r, include=FALSE}
unlink("csvs", force = TRUE, recursive = TRUE)
```
```{r}
save_files <- function(data, group, directory) {
  # get column name from formula
  colname <- as.character(group)[-1]
  
  # Create directory
  dir <- file.path(directory, colname)
  dir_create(dir)

  # split the data
  tmp <- split(data, group)

  # internal file name constructor
  .filename <- function(data){
    # get unique value, make lower, append .csv
    g <- unique(data[[colname]]) |> 
      tolower() |> 
      paste0(".csv")
    # construct file path with directory, grouping and dataset
    file.path(dir, g)
  }
  
  # apply file names to the split data
  # makes `sapply` give a really nice output
  names(tmp) <- sapply(tmp, .filename)

  # write the filees!
  sapply(tmp, function(x) {
    write.csv(x,
              .filename(x),
              row.names = FALSE)
  })
}
save_files(penguins, ~species, "csvs")
save_files(penguins, ~island, "csvs")
list.files("csvs", recursive = TRUE)
```


See? 
Now we have everything I wanted. 
The files are all in neatly ordered folders, named neatly, and it just makes my organisatory heart happy!
Admittedly, it is kind of a large function, but it is also very convenient for quite some stuff I do.
For instance, while I regularly run analyses on complete datasets, some times I need to get some things done in subgroups of the data to inspect possible origins of effects that can be hard when I look at the entire data as a whole. 
And many of the analyses I run are heavy computing, so I need to prepare files to send analyses to a computing cluster. 

This tidbit of code is nice to have to create these datafiles I need.

## 3<sup>rd</sup> of December - Reading in lots of files

Now that we have managed to create lots of files, based on data groupings, let us also see how we can read them in efficiently.
I've made so many absolutely horrid pipelines to do this, before I figured out this way of doing it.

The pre-requisites for this is that all the files you are reading in all have the same columns, if they don't, the last bit will fail. 


```{r}
# list all files in the species folder,
# contiaing the ending "csv" and 
# keep the entire relative path.
list.files("csvs/species", "csv$", full.names = TRUE)
```

We have three files, and we want to read the all in, at once and get them into a list.
We've worked with `lapply()` before, and we will again here. 
We will use the list of file paths in lapply, and run the `read.csv` function on them all.
This should give us a list of three data sets.

```{r}
list.files("csvs/species", "csv$", full.names = TRUE) |> 
  lapply(read.csv)
```

Once that is done, we also want to have them all combined into a single data set, i.e. back to our full penguins data set.
To do that, we will use `do.call` and `rbind` to achieve this.
Now, `do.call` is a bit of magic, and I am not entirely sure of what it does in all contexts.
In this context, it will run through the list, and run `rbind` on each data set, so that we get a single one out.

```{r}
data_list <- list.files("csvs/species", "csv$", full.names = TRUE) |> 
  lapply(read.csv)

do.call(rbind, data_list)
```

And now we have our data frame with 344 rows back!
But! Usually, I would want to know _which file each row comes from_.
In the penguins data here, that is not a huge issue, as the species column basically already tells us that.
But there might be lots of other reasons you'd like to know, for instance for debugging the original data (in case there are suspicious entries), or because the source file information is not inherent in the data. 
To do that, we need a little custom function.

```{r}
merge_files <- function(path, pattern, func = read.csv, ...){
  file_list <- list.files(path, pattern, full.names = TRUE)
  data_list <- lapply(file_list, func, ...)
  # loop through data_list length
  # apply new column with source information
  data_list <- lapply(seq_along(data_list), function(x){
    data_list[[x]]$src <- file_list[x]
    data_list[[x]]
  })
  do.call(rbind, data_list)
}
merged <- merge_files("csvs/species/", "csv")
merged[, c(1:3, 9)]
```

Now we have it all!
The function does quite a lot, in little space, but it also allows quite some customisation.
Like, we can our selves define which `read` function to use, in case the data has a different delimiter than csv, and we can also add any other named argument to that function in our main function call.


## 4<sup>th</sup> of December - fixing column names
I love the [janitor](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html) package.
It has some cleaning functions for data that just make my world so much easier. 
And while janitor's dependencies are small enough that I can often get it when I need, I still have install issues in certain cases.
In those cases, I need to do some simple steps to improve my data dealings.

Depending on how bad things are, there are some small things we can to to help with column naming.
I'm being a little cheeky an borrowing the example data from janitor.
I will not be able to make it _as neat_ as janitor, but we can make it much better!

```{r}
test_df <- as.data.frame(matrix(ncol = 6, nrow = 5))
names(test_df) <- c("firstName", "ábc@!*", "% successful (2009)",
                    "REPEAT VALUE", "REPEAT VALUE", "")

# add some data
test_df[1, ] <- c("jane", "JANE", TRUE, NA, 10, NA)
test_df[2, ] <- c("elleven", "011", FALSE, NA, NA, NA)
test_df[3, ] <- c("Henry", "001", NA, NA, 20, NA)
test_df
```

I think we can all agree this is no fun column names to deal with!
Keeping to base R and some [regular expression](https://www.wikiwand.com/en/Regular_expression) (oh man, I need to google those expressions every time!), we can do a decent bit of cleaning.

```{r}
clean_names <- function(data, col_prefix = "v"){
  colnames <- names(data)
  
  # turn camelCase to snake_case
  colnames <- gsub("(?![A-Z])(\\G(?!^)|\\b[a-zA-Z][a-z]*)([A-Z][a-z]*|\\d+)", 
       "\\1_\\2", colnames, ignore.case = FALSE, perl = TRUE)
  
  # turn white space into _
  colnames <- gsub(" ", "_", colnames)
  
  # turn to lower case
  colnames <- tolower(colnames)
  
  # remove punctuations except _
  colnames <- gsub("[^a-z0-9_]+", "", colnames)
  
  # trim _ from beginning and end
  colnames <- gsub("^_|_$", "", colnames)

  # add column names to columns missing them
  k <- sapply(match("", colnames), function(x){
    colnames[x] <<- paste0(col_prefix, x)
  })
  
  # apply name changes
  names(data) <- colnames
  
  # returned the renamed data
  data
}
test_df <- clean_names(test_df)
test_df
```

Ok, we get pretty close to what I was after.
camelCase turned into snake_case, all lower case, and weird punctuations removed. 
We also manage to name columns without names.
What we miss is that the `á` in "ábc@!*" is removed.
This is because my regular expression is interpreting as a weird special character to remove.
To replace it with an `a` I'd need to get a library that would know how to translate it, and I don't/can't do that. 
So, I'll have to deal with that manually.

## 5<sup>th</sup> of December - removing empty columns

In the type of data I deal with, I do also quite often have to deal with columns containing no data.
Either because the subsetted data are missing a variable, or because a file I read in thinks there is another column, when there truly is not.
I want a nice easy way to deal with that.
Again [janitor](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html) would be my "online" solution, but when offline, I need to deal in my own code.

```{r}
test_df
```

We made a data.frame yesterday with missing values completely from rows 4 and 5, and partial missing data from 2 and 3, while row 1 is the only complete row of data.
And in columns 4 and 6 we are completely missing any data.
We want a simple way to remove all columns that have no information, so we have something simpler to work with.

```{r}
na_rm_col <- function(data){
  # find columns with only missing values
  idx <- apply(data, 2, function(x) all(is.na(x)))
  
  # keep only columns where there is data
  data[, !idx]
}
test_df <- na_rm_col(test_df)
test_df
```

With this function we first apply across the columns (apply dimension 2) and check if all values are `NA`. 
If they are, we make sure we don't return a data.frame with those columns.
The function is neither long nor particularly complicated (though `apply` does take a little time to get the hang of), and its super quick!


## 6<sup>th</sup> of December - removing empty rows
Yesterday we removed empty columns, but we might also need to remove empty rows!
Imagine having subsetted columns, and now, lots of your rows actually don't contain meaningful information any more.
No use in having them around, lets just get rid of them!

The code is _remarkably_ similar to yesterdays code,

```{r}
na_rm_row <- function(data){
  # find columns with only missing values
  idx <- apply(data, 1, function(x) all(is.na(x)))
  
  # keep only rows where there is data
  data[!idx, ]
}
test_df2 <- na_rm_row(test_df)
test_df2
```
We are still using `apply`, but this time along the `1` dimension, which is rows.
And we are using the exact same function inside apply!
Then, we subset the rows with the inverse of that output, giving us only the rows we want.

## 7<sup>th</sup> of December - removing empty rows 2<sup>nd</sup> ed.
In our last post, we removed rows that had all `NA` values, but this is often not the case.
Likely, you'll have some identifier columns that are always populated, and you'll want to make sure you check for `NA` _not_ in those columns.
I.e. we want to discard rows where certain columns only have `NA` not necessarily all!

This one becomes a little trickier!

```{r}
na_rm_row <- function(data, 
                      col_names = names(data),
                      col_inverse = FALSE){
  # Get column index for wanted cols
  col_idx <- names(data) %in% col_names
  
  # Get column names by index
  cols <-  names(data)[col_idx]
  
  # Reverse if you want exclude the columns
  if(col_inverse){
    cols <- names(data)[!col_idx]
  }
    
  # subset the data
  # force output to data.frame
  tmp <- data.frame(data[, cols])

  # find rows with only missing values
  idx <- apply(tmp, 1, function(x) all(is.na(x)))
  
  # keep only rows where there is data
  data[!idx, ]
}
na_rm_row(test_df, 
          col_names = c("successful_2009",
                        "repeat_value"))

na_rm_row(test_df, 
          col_names = c("first_name", "bc"), 
          col_inverse = TRUE)

```
So this function is a little busy. 
Hopefully the code comments help understanding of what is going on. 
I've added to option to either name columns you want to check for `NA`s in, _or_ columns you want **excluded** from that check. 
This way, we can hopefully make it work in any of the circumstances we meet.


